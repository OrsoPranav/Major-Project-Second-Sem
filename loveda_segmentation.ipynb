{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Enhancement of Prithvi via Knowledge Distillation from RemoteCLIP\n",
    "\n",
    "## Task 2: Semantic Segmentation on LoveDA Dataset\n",
    "\n",
    "### Objective\n",
    "Compare baseline Prithvi vs semantically-enhanced (RemoteCLIP-aligned) Prithvi on a **harder** downstream task: pixel-wise semantic segmentation.\n",
    "\n",
    "**Dataset**: LoveDA (Land-cOVEr Domain Adaptive) — high-resolution remote sensing images with 7 land-cover classes.\n",
    "\n",
    "**Previous Task (Classification)**: Model A: 35.78% vs Model B: 40.38% (+4.60%) on NWPU-RESISC45\n",
    "\n",
    "**This Task (Segmentation)**: Compare the two models on dense per-pixel prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the LoveDA Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = Path('LoveDA-dataset')\n",
    "\n",
    "# Count images in each split\n",
    "for split in ['Train', 'Val']:\n",
    "    for domain in ['Rural', 'Urban']:\n",
    "        img_dir = dataset_root / split / domain / 'images_png'\n",
    "        mask_dir = dataset_root / split / domain / 'masks_png'\n",
    "        num_images = len(list(img_dir.glob('*.png')))\n",
    "        num_masks = len(list(mask_dir.glob('*.png')))\n",
    "        print(f\"{split}/{domain}: {num_images} images, {num_masks} masks\")\n",
    "\n",
    "print(f\"\\nTotal Training Images: {len(list((dataset_root / 'Train' / 'Rural' / 'images_png').glob('*.png'))) + len(list((dataset_root / 'Train' / 'Urban' / 'images_png').glob('*.png')))}\")\n",
    "print(f\"Total Validation Images: {len(list((dataset_root / 'Val' / 'Rural' / 'images_png').glob('*.png'))) + len(list((dataset_root / 'Val' / 'Urban' / 'images_png').glob('*.png')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Inspect a Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image from Train/Urban\n",
    "sample_img_path = dataset_root / 'Train' / 'Urban' / 'images_png' / '1366.png'\n",
    "sample_mask_path = dataset_root / 'Train' / 'Urban' / 'masks_png' / '1366.png'\n",
    "\n",
    "sample_image = Image.open(sample_img_path)\n",
    "sample_mask = Image.open(sample_mask_path)\n",
    "\n",
    "# Image properties\n",
    "print(\"=\" * 60)\n",
    "print(\"IMAGE PROPERTIES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"File Path:    {sample_img_path}\")\n",
    "print(f\"Format:       {sample_image.format}\")\n",
    "print(f\"Mode:         {sample_image.mode}\")\n",
    "print(f\"Size (WxH):   {sample_image.size[0]} x {sample_image.size[1]} pixels\")\n",
    "print(f\"Channels:     {len(sample_image.getbands())} ({', '.join(sample_image.getbands())})\")\n",
    "\n",
    "img_array = np.array(sample_image)\n",
    "print(f\"Array Shape:  {img_array.shape}\")\n",
    "print(f\"Dtype:        {img_array.dtype}\")\n",
    "print(f\"Value Range:  [{img_array.min()}, {img_array.max()}]\")\n",
    "print(f\"Mean (RGB):   R={img_array[:,:,0].mean():.1f}, G={img_array[:,:,1].mean():.1f}, B={img_array[:,:,2].mean():.1f}\")\n",
    "\n",
    "# Mask properties\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"MASK PROPERTIES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"File Path:    {sample_mask_path}\")\n",
    "print(f\"Mode:         {sample_mask.mode}\")\n",
    "print(f\"Size (WxH):   {sample_mask.size[0]} x {sample_mask.size[1]} pixels\")\n",
    "\n",
    "mask_array = np.array(sample_mask)\n",
    "print(f\"Array Shape:  {mask_array.shape}\")\n",
    "print(f\"Dtype:        {mask_array.dtype}\")\n",
    "print(f\"Value Range:  [{mask_array.min()}, {mask_array.max()}]\")\n",
    "print(f\"Unique Values: {np.unique(mask_array)}\")\n",
    "\n",
    "# Count pixels per class\n",
    "print(f\"\\nPixel Distribution:\")\n",
    "unique, counts = np.unique(mask_array, return_counts=True)\n",
    "total_pixels = mask_array.size\n",
    "for val, count in zip(unique, counts):\n",
    "    print(f\"  Class {val}: {count:>8,} pixels ({100*count/total_pixels:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the Image and its Segmentation Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoveDA class definitions\n",
    "# Class 0: Background (ignored in evaluation)\n",
    "# Class 1: Building\n",
    "# Class 2: Road\n",
    "# Class 3: Water\n",
    "# Class 4: Barren\n",
    "# Class 5: Forest\n",
    "# Class 6: Agriculture\n",
    "\n",
    "LOVEDA_CLASSES = {\n",
    "    0: 'Background',\n",
    "    1: 'Building',\n",
    "    2: 'Road',\n",
    "    3: 'Water',\n",
    "    4: 'Barren',\n",
    "    5: 'Forest',\n",
    "    6: 'Agriculture'\n",
    "}\n",
    "\n",
    "LOVEDA_COLORS = {\n",
    "    0: [0, 0, 0],         # Background - Black\n",
    "    1: [255, 0, 0],       # Building - Red\n",
    "    2: [255, 255, 0],     # Road - Yellow\n",
    "    3: [0, 0, 255],       # Water - Blue\n",
    "    4: [159, 129, 183],   # Barren - Purple\n",
    "    5: [0, 255, 0],       # Forest - Green\n",
    "    6: [255, 195, 128]    # Agriculture - Orange\n",
    "}\n",
    "\n",
    "def colorize_mask(mask_array):\n",
    "    \"\"\"Convert a single-channel label mask to an RGB color image.\"\"\"\n",
    "    h, w = mask_array.shape\n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for class_id, color in LOVEDA_COLORS.items():\n",
    "        color_mask[mask_array == class_id] = color\n",
    "    return color_mask\n",
    "\n",
    "# Colorize the mask\n",
    "colored_mask = colorize_mask(mask_array)\n",
    "\n",
    "# Create figure with 3 panels: Image, Colored Mask, Overlay\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(img_array)\n",
    "axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Colored segmentation mask\n",
    "axes[1].imshow(colored_mask)\n",
    "axes[1].set_title('Segmentation Mask', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Overlay: image with mask transparency\n",
    "overlay = img_array.copy().astype(np.float32)\n",
    "mask_float = colored_mask.astype(np.float32)\n",
    "alpha = 0.45\n",
    "blended = ((1 - alpha) * overlay + alpha * mask_float).astype(np.uint8)\n",
    "axes[2].imshow(blended)\n",
    "axes[2].set_title('Overlay', fontsize=14, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Add legend\n",
    "present_classes = np.unique(mask_array)\n",
    "legend_patches = []\n",
    "for class_id in present_classes:\n",
    "    color = [c / 255.0 for c in LOVEDA_COLORS[class_id]]\n",
    "    patch = mpatches.Patch(color=color, label=f\"{class_id}: {LOVEDA_CLASSES[class_id]}\")\n",
    "    legend_patches.append(patch)\n",
    "\n",
    "fig.legend(handles=legend_patches, loc='lower center', ncol=len(present_classes),\n",
    "           fontsize=11, frameon=True, fancybox=True, shadow=True,\n",
    "           bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "plt.suptitle(f'LoveDA Sample — {sample_img_path.parent.parent.name} ({sample_img_path.stem}.png)',\n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.08)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Foundation Models\n",
    "\n",
    "Load both models:\n",
    "- **RemoteCLIP** ViT-B/32 (Teacher — frozen) → provides semantic patch-level targets\n",
    "- **Prithvi EO v2** (Student backbone — frozen) → provides patch-level features to align\n",
    "\n",
    "Both models are frozen. Only the spatial projection head (defined later) will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ── Load RemoteCLIP (Teacher) ──────────────────────────────────────────\n",
    "from huggingface_hub import hf_hub_download\n",
    "import open_clip\n",
    "\n",
    "model_name = 'ViT-B-32'\n",
    "checkpoint_path = hf_hub_download(\n",
    "    \"chendelong/RemoteCLIP\", f\"RemoteCLIP-{model_name}.pt\", cache_dir='checkpoints'\n",
    ")\n",
    "print(f\"RemoteCLIP {model_name} downloaded to: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\nCreating model architecture...\")\n",
    "remoteclip_model, _, remoteclip_preprocess = open_clip.create_model_and_transforms(model_name)\n",
    "\n",
    "print(\"Loading RemoteCLIP pretrained weights...\")\n",
    "path_to_checkpoints = 'checkpoints/models--chendelong--RemoteCLIP/snapshots/bf1d8a3ccf2ddbf7c875705e46373bfe542bce38'\n",
    "ckpt = torch.load(f\"{path_to_checkpoints}/RemoteCLIP-{model_name}.pt\", map_location=\"cpu\")\n",
    "load_result = remoteclip_model.load_state_dict(ckpt)\n",
    "print(f\"Load result: {load_result}\")\n",
    "\n",
    "remoteclip_model = remoteclip_model.to(device).eval()\n",
    "for param in remoteclip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(f\"RemoteCLIP loaded and frozen. Output dim: 512\")\n",
    "\n",
    "# ── Load Prithvi EO v2 (Student Backbone) ─────────────────────────────\n",
    "from terratorch.registry import BACKBONE_REGISTRY\n",
    "\n",
    "prithvi_backbone = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_300\")\n",
    "prithvi_backbone = prithvi_backbone.to(device).eval()\n",
    "for param in prithvi_backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "print(f\"Prithvi EO v2 loaded and frozen. Output dim: 1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Patch Token Extraction & Resolution Analysis\n",
    "\n",
    "Segmentation requires **spatial** features, not just a single CLS token.\n",
    "\n",
    "| Model | Patch Size | Grid (224×224 input) | Token Dim |\n",
    "|-------|-----------|----------------------|-----------|\n",
    "| Prithvi EO v2 | 16×16 | 14×14 = 196 tokens | 1024 |\n",
    "| RemoteCLIP ViT-B/32 | 32×32 | 7×7 = 49 tokens | 512 |\n",
    "\n",
    "We extract patch tokens from both, then spatially interpolate RemoteCLIP's 7×7 grid → 14×14 to match Prithvi's resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prithvi_patch_tokens(backbone, images):\n",
    "    \"\"\"\n",
    "    Extract spatial patch tokens from Prithvi (excluding CLS token).\n",
    "    \n",
    "    Input:  images [B, 6, 224, 224]\n",
    "    Output: patch_tokens [B, 1024, 14, 14]\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        features = backbone(images)\n",
    "        # features[-1] shape: [B, 197, 1024] → 1 CLS + 196 patch tokens\n",
    "        patch_tokens = features[-1][:, 1:, :]  # [B, 196, 1024] — drop CLS\n",
    "        \n",
    "        B, N, C = patch_tokens.shape\n",
    "        h = w = int(N ** 0.5)  # 14\n",
    "        patch_tokens = patch_tokens.permute(0, 2, 1).reshape(B, C, h, w)  # [B, 1024, 14, 14]\n",
    "    return patch_tokens\n",
    "\n",
    "\n",
    "def extract_remoteclip_patch_tokens(model, images):\n",
    "    \"\"\"\n",
    "    Extract spatial patch tokens from RemoteCLIP ViT-B/32 (excluding CLS token).\n",
    "    \n",
    "    Input:  images [B, 3, 224, 224]\n",
    "    Output: patch_tokens [B, 512, 7, 7]\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        visual = model.visual\n",
    "        # Manual forward pass to get intermediate patch tokens\n",
    "        x = visual.conv1(images)  # [B, 768, 7, 7]  (32x32 patches on 224x224)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # [B, 768, 49]\n",
    "        x = x.permute(0, 2, 1)  # [B, 49, 768]\n",
    "        \n",
    "        # Add class embedding and positional embedding\n",
    "        cls_token = visual.class_embedding.unsqueeze(0).unsqueeze(0).expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)  # [B, 50, 768]\n",
    "        x = x + visual.positional_embedding.unsqueeze(0)\n",
    "        \n",
    "        # Pre-LN\n",
    "        x = visual.ln_pre(x)\n",
    "        \n",
    "        # Transformer\n",
    "        x = x.permute(1, 0, 2)  # [50, B, 768] (seq_first for transformer)\n",
    "        x = visual.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # [B, 50, 768]\n",
    "        \n",
    "        # Post-LN on all tokens\n",
    "        x = visual.ln_post(x)\n",
    "        \n",
    "        # Project all tokens (not just CLS) through the output projection\n",
    "        if visual.proj is not None:\n",
    "            x = x @ visual.proj  # [B, 50, 512]\n",
    "        \n",
    "        # Take patch tokens (drop CLS at position 0)\n",
    "        patch_tokens = x[:, 1:, :]  # [B, 49, 512]\n",
    "        \n",
    "        # L2 normalize each token\n",
    "        patch_tokens = patch_tokens / patch_tokens.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        B, N, C = patch_tokens.shape\n",
    "        h = w = int(N ** 0.5)  # 7\n",
    "        patch_tokens = patch_tokens.permute(0, 2, 1).reshape(B, C, h, w)  # [B, 512, 7, 7]\n",
    "    return patch_tokens\n",
    "\n",
    "\n",
    "# ── Verify on sample image ────────────────────────────────────────────\n",
    "sample_img = Image.open(dataset_root / 'Train' / 'Urban' / 'images_png' / '1366.png').convert('RGB')\n",
    "\n",
    "# Prepare for Prithvi (6-channel, zero-padded)\n",
    "prithvi_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "prithvi_input = prithvi_tf(sample_img).unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "prithvi_input = torch.cat([prithvi_input, torch.zeros_like(prithvi_input)], dim=1)  # [1, 6, 224, 224]\n",
    "\n",
    "# Prepare for RemoteCLIP (uses its own preprocessing)\n",
    "remoteclip_input = remoteclip_preprocess(sample_img).unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "\n",
    "# Extract patch tokens\n",
    "prithvi_patches = extract_prithvi_patch_tokens(prithvi_backbone, prithvi_input)\n",
    "remoteclip_patches = extract_remoteclip_patch_tokens(remoteclip_model, remoteclip_input)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PATCH TOKEN SHAPES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prithvi patch tokens:    {prithvi_patches.shape}\")\n",
    "print(f\"  → {prithvi_patches.shape[1]}-dim features on a {prithvi_patches.shape[2]}×{prithvi_patches.shape[3]} grid\")\n",
    "print(f\"\\nRemoteCLIP patch tokens: {remoteclip_patches.shape}\")\n",
    "print(f\"  → {remoteclip_patches.shape[1]}-dim features on a {remoteclip_patches.shape[2]}×{remoteclip_patches.shape[3]} grid\")\n",
    "\n",
    "# Demonstrate spatial interpolation: 7x7 → 14x14\n",
    "remoteclip_upsampled = F.interpolate(\n",
    "    remoteclip_patches, size=(14, 14), mode='bilinear', align_corners=False\n",
    ")\n",
    "print(f\"\\nRemoteCLIP after interpolation: {remoteclip_upsampled.shape}\")\n",
    "print(f\"  → Now matches Prithvi's 14×14 spatial grid\")\n",
    "\n",
    "# Visualize the feature maps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Prithvi: mean activation across channels\n",
    "prithvi_vis = prithvi_patches[0].mean(dim=0).cpu().numpy()\n",
    "axes[0].imshow(prithvi_vis, cmap='viridis')\n",
    "axes[0].set_title(f'Prithvi Patch Features\\n(mean of 1024 channels, 14×14)', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# RemoteCLIP original: mean activation\n",
    "remoteclip_vis = remoteclip_patches[0].mean(dim=0).cpu().numpy()\n",
    "axes[1].imshow(remoteclip_vis, cmap='magma')\n",
    "axes[1].set_title(f'RemoteCLIP Patch Features\\n(mean of 512 channels, 7×7)', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# RemoteCLIP upsampled\n",
    "remoteclip_up_vis = remoteclip_upsampled[0].mean(dim=0).cpu().numpy()\n",
    "axes[2].imshow(remoteclip_up_vis, cmap='magma')\n",
    "axes[2].set_title(f'RemoteCLIP Upsampled\\n(interpolated to 14×14)', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('Spatial Feature Maps from Both Models', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Spatial Projection Head\n",
    "\n",
    "A 2-layer MLP applied **per spatial position** (1×1 convolutions) to map Prithvi's 1024-dim patch features to RemoteCLIP's 512-dim semantic space.\n",
    "\n",
    "This is equivalent to applying the same linear transformation independently at every position in the 14×14 grid — no spatial mixing, purely channel-wise projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialProjectionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Per-position MLP using 1x1 convolutions.\n",
    "    Maps Prithvi features (1024-dim) → RemoteCLIP space (512-dim)\n",
    "    at every spatial location independently.\n",
    "    \n",
    "    Input:  [B, 1024, H, W]\n",
    "    Output: [B, 512,  H, W]\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1024, hidden_dim=768, output_dim=512):\n",
    "        super(SpatialProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, hidden_dim, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(hidden_dim, output_dim, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "spatial_proj_head = SpatialProjectionHead(input_dim=1024, hidden_dim=768, output_dim=512).to(device)\n",
    "\n",
    "# Verify\n",
    "total_params = sum(p.numel() for p in spatial_proj_head.parameters())\n",
    "trainable_params = sum(p.numel() for p in spatial_proj_head.parameters() if p.requires_grad)\n",
    "print(\"=\" * 60)\n",
    "print(\"SPATIAL PROJECTION HEAD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Architecture: Conv1x1(1024→768) → GELU → Dropout(0.1) → Conv1x1(768→512)\")\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Quick shape verification\n",
    "dummy_input = torch.randn(2, 1024, 14, 14).to(device)\n",
    "dummy_output = spatial_proj_head(dummy_input)\n",
    "print(f\"\\nShape check: {dummy_input.shape} → {dummy_output.shape}\")\n",
    "print(f\"Per-position: 1024-dim → 512-dim at each of the 14×14 positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataset for Phase 1: Unsupervised Spatial Alignment\n",
    "\n",
    "For distillation we only need **images** (no masks). We use all LoveDA training images (Rural + Urban combined) and apply dual transforms — one for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoveDA_Unsupervised(Dataset):\n",
    "    \"\"\"\n",
    "    LoveDA images only (no masks) for unsupervised spatial alignment.\n",
    "    Combines Rural + Urban from the training split.\n",
    "    Returns dual-transformed images for Prithvi and RemoteCLIP.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, prithvi_transform, remoteclip_transform, split='Train'):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.prithvi_transform = prithvi_transform\n",
    "        self.remoteclip_transform = remoteclip_transform\n",
    "        \n",
    "        self.image_paths = []\n",
    "        for domain in ['Rural', 'Urban']:\n",
    "            img_dir = self.root_dir / split / domain / 'images_png'\n",
    "            self.image_paths.extend(sorted(img_dir.glob('*.png')))\n",
    "        \n",
    "        print(f\"[Phase 1 Dataset] Found {len(self.image_paths)} images for unsupervised alignment\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        prithvi_img = self.prithvi_transform(image)\n",
    "        remoteclip_img = self.remoteclip_transform(image)\n",
    "        return prithvi_img, remoteclip_img\n",
    "\n",
    "\n",
    "# Prithvi transform (3-channel; we zero-pad to 6-ch in the extraction function)\n",
    "prithvi_transform_phase1 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# RemoteCLIP uses its own preprocess pipeline\n",
    "phase1_dataset = LoveDA_Unsupervised(\n",
    "    'LoveDA-dataset',\n",
    "    prithvi_transform=prithvi_transform_phase1,\n",
    "    remoteclip_transform=remoteclip_preprocess,\n",
    "    split='Train'\n",
    ")\n",
    "\n",
    "phase1_loader = DataLoader(\n",
    "    phase1_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Phase 1 batches per epoch: {len(phase1_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PHASE 1: Spatial Knowledge Distillation Training\n",
    "\n",
    "Train the Spatial Projection Head to align Prithvi's patch-level features with RemoteCLIP's.\n",
    "\n",
    "**Pipeline per image:**\n",
    "1. Extract Prithvi patch tokens → `[B, 1024, 14, 14]`\n",
    "2. Extract RemoteCLIP patch tokens → `[B, 512, 7, 7]` → interpolate to `[B, 512, 14, 14]`\n",
    "3. Project Prithvi tokens through Spatial Projection Head → `[B, 512, 14, 14]`\n",
    "4. **Per-position cosine embedding loss** between projected Prithvi and interpolated RemoteCLIP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_cosine_loss(student_features, teacher_features):\n",
    "    \"\"\"\n",
    "    Per-position cosine embedding loss.\n",
    "    \n",
    "    student_features: [B, C, H, W]\n",
    "    teacher_features: [B, C, H, W]\n",
    "    \n",
    "    Returns: scalar loss = mean(1 - cosine_sim) over all positions and batch\n",
    "    \"\"\"\n",
    "    # L2 normalize along channel dimension\n",
    "    student_norm = F.normalize(student_features, p=2, dim=1)\n",
    "    teacher_norm = F.normalize(teacher_features, p=2, dim=1)\n",
    "    \n",
    "    # Per-position cosine similarity: [B, H, W]\n",
    "    cosine_sim = (student_norm * teacher_norm).sum(dim=1)\n",
    "    \n",
    "    # Loss: minimize (1 - cosine_sim)\n",
    "    loss = (1 - cosine_sim).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training configuration\n",
    "phase1_epochs = 15\n",
    "phase1_optimizer = optim.AdamW(spatial_proj_head.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "phase1_scheduler = optim.lr_scheduler.CosineAnnealingLR(phase1_optimizer, T_max=phase1_epochs, eta_min=1e-6)\n",
    "\n",
    "phase1_history = {'loss': [], 'cosine_sim': []}\n",
    "best_phase1_loss = float('inf')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: SPATIAL KNOWLEDGE DISTILLATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Objective: Align Prithvi patch tokens with RemoteCLIP patch tokens\")\n",
    "print(f\"Teacher:   RemoteCLIP ViT-B/32 (frozen) — 512-dim, 7×7 grid\")\n",
    "print(f\"Student:   Prithvi + Spatial Projection Head — 1024→512, 14×14 grid\")\n",
    "print(f\"Loss:      Per-position cosine embedding loss\")\n",
    "print(f\"Epochs:    {phase1_epochs}\")\n",
    "print(f\"Optimizer: AdamW (lr=1e-3, wd=0.01)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 Training Loop\n",
    "for epoch in range(phase1_epochs):\n",
    "    spatial_proj_head.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_cosine_sim = 0.0\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EPOCH {epoch+1}/{phase1_epochs}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    with tqdm(phase1_loader, desc='Distillation', ncols=100) as pbar:\n",
    "        for batch_idx, (prithvi_imgs, remoteclip_imgs) in enumerate(pbar):\n",
    "            prithvi_imgs = prithvi_imgs.to(device)\n",
    "            remoteclip_imgs = remoteclip_imgs.to(device)\n",
    "            \n",
    "            # Zero-pad Prithvi input to 6 channels\n",
    "            prithvi_6ch = torch.cat([prithvi_imgs, torch.zeros_like(prithvi_imgs)], dim=1)\n",
    "            \n",
    "            # Extract spatial features from both frozen models\n",
    "            prithvi_patches = extract_prithvi_patch_tokens(prithvi_backbone, prithvi_6ch)     # [B, 1024, 14, 14]\n",
    "            remoteclip_patches = extract_remoteclip_patch_tokens(remoteclip_model, remoteclip_imgs)  # [B, 512, 7, 7]\n",
    "            \n",
    "            # Interpolate RemoteCLIP 7×7 → 14×14\n",
    "            teacher_targets = F.interpolate(\n",
    "                remoteclip_patches, size=(14, 14), mode='bilinear', align_corners=False\n",
    "            )  # [B, 512, 14, 14]\n",
    "            \n",
    "            # Project Prithvi features\n",
    "            student_projected = spatial_proj_head(prithvi_patches)  # [B, 512, 14, 14]\n",
    "            \n",
    "            # Compute per-position cosine loss\n",
    "            loss = spatial_cosine_loss(student_projected, teacher_targets)\n",
    "            \n",
    "            # Backward\n",
    "            phase1_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            phase1_optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                s_norm = F.normalize(student_projected, p=2, dim=1)\n",
    "                t_norm = F.normalize(teacher_targets, p=2, dim=1)\n",
    "                cosine_sim = (s_norm * t_norm).sum(dim=1).mean().item()\n",
    "                epoch_cosine_sim += cosine_sim\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'CosSim': f'{cosine_sim:.4f}'\n",
    "            })\n",
    "    \n",
    "    phase1_scheduler.step()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(phase1_loader)\n",
    "    avg_cosine_sim = epoch_cosine_sim / len(phase1_loader)\n",
    "    \n",
    "    phase1_history['loss'].append(avg_loss)\n",
    "    phase1_history['cosine_sim'].append(avg_cosine_sim)\n",
    "    \n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"EPOCH {epoch+1} SUMMARY:\")\n",
    "    print(f\"  Avg Loss:              {avg_loss:.4f}\")\n",
    "    print(f\"  Avg Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "    print(f\"  Learning Rate:         {phase1_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_phase1_loss:\n",
    "        best_phase1_loss = avg_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'spatial_proj_head_state_dict': spatial_proj_head.state_dict(),\n",
    "            'optimizer_state_dict': phase1_optimizer.state_dict(),\n",
    "            'loss': best_phase1_loss,\n",
    "            'cosine_sim': avg_cosine_sim\n",
    "        }, 'best_spatial_projection_head.pth')\n",
    "        print(f\"Saved best spatial projection head (Loss: {best_phase1_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"PHASE 1 COMPLETED! Best Loss: {best_phase1_loss:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Phase 1 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(range(1, len(phase1_history['loss'])+1), phase1_history['loss'],\n",
    "             marker='o', color='crimson', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (1 - cos_sim)', fontsize=12)\n",
    "axes[0].set_title('Phase 1: Spatial Distillation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cosine similarity curve\n",
    "axes[1].plot(range(1, len(phase1_history['cosine_sim'])+1), phase1_history['cosine_sim'],\n",
    "             marker='s', color='seagreen', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Cosine Similarity', fontsize=12)\n",
    "axes[1].set_title('Phase 1: Spatial Alignment Quality', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0.0, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load Best Spatial Projection Head & Freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best spatial projection head from Phase 1\n",
    "checkpoint = torch.load('best_spatial_projection_head.pth')\n",
    "spatial_proj_head.load_state_dict(checkpoint['spatial_proj_head_state_dict'])\n",
    "spatial_proj_head.eval()\n",
    "\n",
    "# Freeze for Phase 2\n",
    "for param in spatial_proj_head.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Loaded best spatial projection head from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"  Final Loss:              {checkpoint['loss']:.4f}\")\n",
    "print(f\"  Final Cosine Similarity: {checkpoint['cosine_sim']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Define Segmentation Decoder\n",
    "\n",
    "A lightweight convolutional decoder that takes a spatial feature grid (14×14) and upsamples it to the target segmentation resolution (512×512).\n",
    "\n",
    "**Architecture**: Progressive upsampling with Conv → BatchNorm → ReLU blocks.\n",
    "\n",
    "The **same decoder architecture** is used for both models — only the input channel count differs:\n",
    "- **Model A** (Baseline): 1024 input channels (raw Prithvi features)\n",
    "- **Model B** (Enhanced): 512 input channels (projected features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight decoder: upsamples from 14×14 feature grid → 512×512 segmentation map.\n",
    "    \n",
    "    Upsampling stages:\n",
    "        14×14 → 28×28 → 56×56 → 112×112 → 224×224 → 512×512\n",
    "    \n",
    "    Each stage: Upsample(2×) → Conv3x3 → BN → ReLU\n",
    "    Final: Conv1x1 → num_classes\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_classes=7):\n",
    "        super(SegmentationDecoder, self).__init__()\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            # 14×14 → 28×28\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 28×28 → 56×56\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 56×56 → 112×112\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 112×112 → 224×224\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 224×224 → 512×512\n",
    "            nn.Upsample(size=(512, 512), mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Final classification head\n",
    "        self.classifier = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x  # [B, num_classes, 512, 512]\n",
    "\n",
    "\n",
    "# Verify decoder architecture\n",
    "decoder_a = SegmentationDecoder(in_channels=1024, num_classes=7)\n",
    "decoder_b = SegmentationDecoder(in_channels=512, num_classes=7)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SEGMENTATION DECODER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFor Model A (Baseline):   in_channels=1024\")\n",
    "print(f\"  Trainable params: {sum(p.numel() for p in decoder_a.parameters() if p.requires_grad):,}\")\n",
    "print(f\"\\nFor Model B (Enhanced):   in_channels=512\")\n",
    "print(f\"  Trainable params: {sum(p.numel() for p in decoder_b.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Shape check\n",
    "dummy_a = torch.randn(1, 1024, 14, 14)\n",
    "dummy_b = torch.randn(1, 512, 14, 14)\n",
    "out_a = decoder_a(dummy_a)\n",
    "out_b = decoder_b(dummy_b)\n",
    "print(f\"\\nShape checks:\")\n",
    "print(f\"  Model A: {dummy_a.shape} → {out_a.shape}\")\n",
    "print(f\"  Model B: {dummy_b.shape} → {out_b.shape}\")\n",
    "\n",
    "# Clean up verification decoders\n",
    "del decoder_a, decoder_b, dummy_a, dummy_b, out_a, out_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Define Model A (Baseline) and Model B (Semantic-Enhanced)\n",
    "\n",
    "### Model A (Baseline)\n",
    "`Image → Prithvi (frozen) → patch tokens [B, 1024, 14, 14] → Decoder → [B, 7, 512, 512]`\n",
    "\n",
    "### Model B (Semantic-Enhanced)\n",
    "`Image → Prithvi (frozen) → patch tokens [B, 1024, 14, 14] → Spatial PH (frozen) → [B, 512, 14, 14] → Decoder → [B, 7, 512, 512]`\n",
    "\n",
    "Only the decoder weights are trainable in both cases — ensuring a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegModelA_Baseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline segmentation: Prithvi (frozen) → Decoder (trainable)\n",
    "    Feature channel dim: 1024\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, num_classes=7):\n",
    "        super(SegModelA_Baseline, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.decoder = SegmentationDecoder(in_channels=1024, num_classes=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Zero-pad RGB → 6 channels\n",
    "        if x.shape[1] == 3:\n",
    "            x = torch.cat([x, torch.zeros_like(x)], dim=1)\n",
    "        \n",
    "        # Frozen backbone → patch tokens\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "            patch_tokens = features[-1][:, 1:, :]  # [B, 196, 1024]\n",
    "            B, N, C = patch_tokens.shape\n",
    "            h = w = int(N ** 0.5)\n",
    "            patch_tokens = patch_tokens.permute(0, 2, 1).reshape(B, C, h, w)  # [B, 1024, 14, 14]\n",
    "        \n",
    "        patch_tokens = patch_tokens.detach()  # Detach from no_grad context to avoid cuDNN stream issues\n",
    "        \n",
    "        # Trainable decoder\n",
    "        logits = self.decoder(patch_tokens)  # [B, 7, 512, 512]\n",
    "        return logits\n",
    "\n",
    "\n",
    "class SegModelB_SemanticEnhanced(nn.Module):\n",
    "    \"\"\"\n",
    "    Semantic-enhanced segmentation: Prithvi (frozen) → Spatial PH (frozen) → Decoder (trainable)\n",
    "    Feature channel dim: 512\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, spatial_proj_head, num_classes=7):\n",
    "        super(SegModelB_SemanticEnhanced, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.spatial_proj_head = spatial_proj_head\n",
    "        self.decoder = SegmentationDecoder(in_channels=512, num_classes=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Zero-pad RGB → 6 channels\n",
    "        if x.shape[1] == 3:\n",
    "            x = torch.cat([x, torch.zeros_like(x)], dim=1)\n",
    "        \n",
    "        # Frozen backbone → patch tokens\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "            patch_tokens = features[-1][:, 1:, :]  # [B, 196, 1024]\n",
    "            B, N, C = patch_tokens.shape\n",
    "            h = w = int(N ** 0.5)\n",
    "            patch_tokens = patch_tokens.permute(0, 2, 1).reshape(B, C, h, w)  # [B, 1024, 14, 14]\n",
    "            \n",
    "            # Frozen spatial projection\n",
    "            projected = self.spatial_proj_head(patch_tokens)  # [B, 512, 14, 14]\n",
    "        \n",
    "        projected = projected.detach()  # Detach from no_grad context to avoid cuDNN stream issues\n",
    "        \n",
    "        # Trainable decoder\n",
    "        logits = self.decoder(projected)  # [B, 7, 512, 512]\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Instantiate both models\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "seg_model_a = SegModelA_Baseline(prithvi_backbone, num_classes=NUM_CLASSES).to(device)\n",
    "seg_model_b = SegModelB_SemanticEnhanced(prithvi_backbone, spatial_proj_head, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SEGMENTATION MODELS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel A (Baseline):\")\n",
    "print(f\"  Pipeline: Prithvi(frozen) → Decoder(1024→7)\")\n",
    "print(f\"  Trainable params: {sum(p.numel() for p in seg_model_a.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "print(f\"\\nModel B (Semantic-Enhanced):\")\n",
    "print(f\"  Pipeline: Prithvi(frozen) → SpatialPH(frozen) → Decoder(512→7)\")\n",
    "print(f\"  Trainable params: {sum(p.numel() for p in seg_model_b.parameters() if p.requires_grad):,}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Dataset & DataLoader for Phase 2: Segmentation\n",
    "\n",
    "LoveDA images with their segmentation masks. Images are resized to 512×512 (both image and mask).\n",
    "\n",
    "**Class mapping**: Class 0 (Background) is **ignored** during loss computation using `ignore_index=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoveDA_Segmentation(Dataset):\n",
    "    \"\"\"\n",
    "    LoveDA dataset for semantic segmentation.\n",
    "    Returns (image_tensor, mask_tensor) pairs.\n",
    "    \n",
    "    Images: resized to img_size, normalized\n",
    "    Masks:  resized to mask_size using nearest-neighbor (preserves class labels)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, split='Train', img_size=224, mask_size=512, augment=False):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.img_size = img_size\n",
    "        self.mask_size = mask_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "        \n",
    "        for domain in ['Rural', 'Urban']:\n",
    "            img_dir = self.root_dir / split / domain / 'images_png'\n",
    "            mask_dir = self.root_dir / split / domain / 'masks_png'\n",
    "            \n",
    "            for img_path in sorted(img_dir.glob('*.png')):\n",
    "                mask_path = mask_dir / img_path.name\n",
    "                if mask_path.exists():\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_paths.append(mask_path)\n",
    "        \n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        \n",
    "        print(f\"[{split}] Found {len(self.image_paths)} image-mask pairs\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        mask = Image.open(self.mask_paths[idx])\n",
    "        \n",
    "        # Resize\n",
    "        image = image.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
    "        mask = mask.resize((self.mask_size, self.mask_size), Image.NEAREST)\n",
    "        \n",
    "        # Random augmentations (applied consistently to image and mask)\n",
    "        if self.augment:\n",
    "            # Random horizontal flip\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            # Random vertical flip\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                mask = mask.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        image = transforms.ToTensor()(image)  # [3, img_size, img_size]\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        mask = torch.from_numpy(np.array(mask)).long()  # [mask_size, mask_size]\n",
    "        \n",
    "        # Remap class 7 (\"no data\") to 0 (background) so it is ignored by CrossEntropyLoss\n",
    "        mask[mask == 7] = 0\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_seg_dataset = LoveDA_Segmentation('LoveDA-dataset', split='Train', img_size=224, mask_size=512, augment=True)\n",
    "val_seg_dataset = LoveDA_Segmentation('LoveDA-dataset', split='Val', img_size=224, mask_size=512, augment=False)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size_seg = 4\n",
    "train_seg_loader = DataLoader(train_seg_dataset, batch_size=batch_size_seg, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_seg_loader = DataLoader(val_seg_dataset, batch_size=batch_size_seg, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"\\nPhase 2 Segmentation Data:\")\n",
    "print(f\"  Train: {len(train_seg_dataset)} samples, {len(train_seg_loader)} batches\")\n",
    "print(f\"  Val:   {len(val_seg_dataset)} samples, {len(val_seg_loader)} batches\")\n",
    "print(f\"  Image size: 224×224 (input to backbone)\")\n",
    "print(f\"  Mask size:  512×512 (decoder output)\")\n",
    "print(f\"  Batch size: {batch_size_seg}\")\n",
    "\n",
    "# Verify a sample\n",
    "sample_img, sample_mask = train_seg_dataset[0]\n",
    "print(f\"\\nSample shapes: image={sample_img.shape}, mask={sample_mask.shape}\")\n",
    "print(f\"Mask unique values: {torch.unique(sample_mask).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Segmentation Training Function\n",
    "\n",
    "Shared training loop for both models. Uses:\n",
    "- **CrossEntropyLoss** with `ignore_index=0` (Background)\n",
    "- **mIoU** (mean Intersection-over-Union) as the primary metric\n",
    "- **Per-class IoU** tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_miou(preds, labels, num_classes=7, ignore_index=0):\n",
    "    \"\"\"\n",
    "    Compute per-class IoU and mean IoU (excluding background class 0).\n",
    "    \n",
    "    preds:  [B, H, W] — predicted class indices\n",
    "    labels: [B, H, W] — ground truth class indices\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    for cls in range(1, num_classes):  # Skip class 0 (Background)\n",
    "        pred_mask = (preds == cls)\n",
    "        label_mask = (labels == cls)\n",
    "        \n",
    "        intersection = (pred_mask & label_mask).sum().item()\n",
    "        union = (pred_mask | label_mask).sum().item()\n",
    "        \n",
    "        if union == 0:\n",
    "            continue  # Class not present in this batch\n",
    "        \n",
    "        ious.append(intersection / union)\n",
    "    \n",
    "    if len(ious) == 0:\n",
    "        return 0.0, {}\n",
    "    \n",
    "    miou = np.mean(ious)\n",
    "    return miou, ious\n",
    "\n",
    "\n",
    "def train_segmentation(model, train_loader, val_loader, num_epochs=25, lr=1e-3, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Train a segmentation model and track mIoU.\n",
    "    Only decoder parameters are trained (backbone and projection head are frozen).\n",
    "    \"\"\"\n",
    "    # Only train parameters that require grad (decoder only)\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore background\n",
    "    optimizer = optim.AdamW(trainable_params, lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_miou': [], 'val_overall_acc': []}\n",
    "    best_val_miou = 0.0\n",
    "    best_state = None\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
    "    print(f\"Epochs: {num_epochs}, LR: {lr}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ── Training ──────────────────────────────────────────────────\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', ncols=100) as pbar:\n",
    "            for images, masks in pbar:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)  # [B, 512, 512]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                logits = model(images)  # [B, 7, 512, 512]\n",
    "                loss = criterion(logits, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # ── Validation ────────────────────────────────────────────────\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_ious = []\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Predictions\n",
    "                preds = logits.argmax(dim=1)  # [B, 512, 512]\n",
    "                \n",
    "                # mIoU\n",
    "                miou_batch, _ = compute_miou(preds.cpu(), masks.cpu(), num_classes=7)\n",
    "                all_ious.append(miou_batch)\n",
    "                \n",
    "                # Overall accuracy (excluding background)\n",
    "                valid_mask = masks != 0\n",
    "                val_correct += (preds[valid_mask] == masks[valid_mask]).sum().item()\n",
    "                val_total += valid_mask.sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_miou = np.mean(all_ious) if all_ious else 0.0\n",
    "        val_acc = 100.0 * val_correct / val_total if val_total > 0 else 0.0\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_miou'].append(val_miou)\n",
    "        history['val_overall_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Val mIoU: {val_miou:.4f} | \"\n",
    "              f\"Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Track best\n",
    "        if val_miou > best_val_miou:\n",
    "            best_val_miou = val_miou\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "            print(f\"  >> New best mIoU: {best_val_miou:.4f}\")\n",
    "    \n",
    "    # Load best model weights\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    print(f\"\\nBest Validation mIoU: {best_val_miou:.4f}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    return history, best_val_miou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Train Model A (Baseline Segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_seg_a, best_miou_a = train_segmentation(\n",
    "    seg_model_a,\n",
    "    train_seg_loader,\n",
    "    val_seg_loader,\n",
    "    num_epochs=25,\n",
    "    lr=1e-3,\n",
    "    model_name=\"Model A (Baseline Segmentation)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Train Model B (Semantic-Enhanced Segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_seg_b, best_miou_b = train_segmentation(\n",
    "    seg_model_b,\n",
    "    train_seg_loader,\n",
    "    val_seg_loader,\n",
    "    num_epochs=25,\n",
    "    lr=1e-3,\n",
    "    model_name=\"Model B (Semantic-Enhanced Segmentation)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Compare Results: Model A vs Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SEGMENTATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel A (Baseline — Raw Prithvi Features):\")\n",
    "print(f\"  Best Val mIoU:       {best_miou_a:.4f}\")\n",
    "print(f\"  Best Val Accuracy:   {max(history_seg_a['val_overall_acc']):.2f}%\")\n",
    "\n",
    "print(f\"\\nModel B (Semantic-Enhanced — RemoteCLIP Aligned):\")\n",
    "print(f\"  Best Val mIoU:       {best_miou_b:.4f}\")\n",
    "print(f\"  Best Val Accuracy:   {max(history_seg_b['val_overall_acc']):.2f}%\")\n",
    "\n",
    "miou_improvement = best_miou_b - best_miou_a\n",
    "acc_improvement = max(history_seg_b['val_overall_acc']) - max(history_seg_a['val_overall_acc'])\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  mIoU:     {miou_improvement:+.4f} ({miou_improvement/best_miou_a*100:+.2f}% relative)\")\n",
    "print(f\"  Accuracy: {acc_improvement:+.2f}%\")\n",
    "\n",
    "if best_miou_b > best_miou_a:\n",
    "    print(f\"\\nSemantic enhancement improved segmentation performance!\")\n",
    "else:\n",
    "    print(f\"\\nBaseline performed better on segmentation. See analysis below.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Training Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "epochs = range(1, len(history_seg_a['train_loss']) + 1)\n",
    "\n",
    "# Training Loss\n",
    "axes[0, 0].plot(epochs, history_seg_a['train_loss'], 'o-', label='Model A (Baseline)', linewidth=2, markersize=4)\n",
    "axes[0, 0].plot(epochs, history_seg_b['train_loss'], 's-', label='Model B (Semantic)', linewidth=2, markersize=4)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "axes[0, 1].plot(epochs, history_seg_a['val_loss'], 'o-', label='Model A (Baseline)', linewidth=2, markersize=4)\n",
    "axes[0, 1].plot(epochs, history_seg_b['val_loss'], 's-', label='Model B (Semantic)', linewidth=2, markersize=4)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation mIoU\n",
    "axes[1, 0].plot(epochs, history_seg_a['val_miou'], 'o-', label='Model A (Baseline)', linewidth=2, markersize=4, color='orange')\n",
    "axes[1, 0].plot(epochs, history_seg_b['val_miou'], 's-', label='Model B (Semantic)', linewidth=2, markersize=4, color='green')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('mIoU', fontsize=12)\n",
    "axes[1, 0].set_title('Validation mIoU', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=best_miou_a, color='orange', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].axhline(y=best_miou_b, color='green', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Validation Overall Accuracy\n",
    "axes[1, 1].plot(epochs, history_seg_a['val_overall_acc'], 'o-', label='Model A (Baseline)', linewidth=2, markersize=4, color='orange')\n",
    "axes[1, 1].plot(epochs, history_seg_b['val_overall_acc'], 's-', label='Model B (Semantic)', linewidth=2, markersize=4, color='green')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Validation Overall Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Phase 2: Segmentation Training Comparison', fontsize=16, fontweight='bold', y=1.002)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Per-Class IoU Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmentation(model, val_loader, num_classes=7, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Full evaluation: per-class IoU, mIoU, and pixel accuracy.\n",
    "    Uses confusion matrix for accurate computation over entire validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Confusion matrix: [num_classes, num_classes]\n",
    "    confusion = torch.zeros(num_classes, num_classes, dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=f'Evaluating {model_name}'):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            logits = model(images)\n",
    "            preds = logits.argmax(dim=1).cpu()  # [B, 512, 512]\n",
    "            masks_cpu = masks.cpu()\n",
    "            \n",
    "            # Update confusion matrix\n",
    "            for cls_true in range(num_classes):\n",
    "                for cls_pred in range(num_classes):\n",
    "                    confusion[cls_true, cls_pred] += ((masks_cpu == cls_true) & (preds == cls_pred)).sum().item()\n",
    "    \n",
    "    # Per-class IoU (skip class 0 = Background)\n",
    "    per_class_iou = {}\n",
    "    for cls in range(1, num_classes):\n",
    "        tp = confusion[cls, cls].item()\n",
    "        fp = confusion[:, cls].sum().item() - tp\n",
    "        fn = confusion[cls, :].sum().item() - tp\n",
    "        \n",
    "        if tp + fp + fn == 0:\n",
    "            per_class_iou[cls] = float('nan')\n",
    "        else:\n",
    "            per_class_iou[cls] = tp / (tp + fp + fn)\n",
    "    \n",
    "    # mIoU (excluding NaN classes)\n",
    "    valid_ious = [v for v in per_class_iou.values() if not np.isnan(v)]\n",
    "    miou = np.mean(valid_ious) if valid_ious else 0.0\n",
    "    \n",
    "    # Overall pixel accuracy (excluding background)\n",
    "    correct = sum(confusion[c, c].item() for c in range(1, num_classes))\n",
    "    total = sum(confusion[c, :].sum().item() for c in range(1, num_classes))\n",
    "    overall_acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name} — Segmentation Evaluation\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Class':<15} {'IoU':>8}\")\n",
    "    print(f\"{'-'*25}\")\n",
    "    for cls_id, iou in per_class_iou.items():\n",
    "        cls_name = LOVEDA_CLASSES[cls_id]\n",
    "        if np.isnan(iou):\n",
    "            print(f\"{cls_name:<15} {'N/A':>8}\")\n",
    "        else:\n",
    "            print(f\"{cls_name:<15} {iou:>8.4f}\")\n",
    "    print(f\"{'-'*25}\")\n",
    "    print(f\"{'mIoU':<15} {miou:>8.4f}\")\n",
    "    print(f\"{'Pixel Acc':<15} {overall_acc:>7.2f}%\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return per_class_iou, miou, overall_acc\n",
    "\n",
    "\n",
    "# Evaluate both models\n",
    "per_class_iou_a, miou_a_final, acc_a_final = evaluate_segmentation(\n",
    "    seg_model_a, val_seg_loader, model_name=\"Model A (Baseline)\")\n",
    "\n",
    "per_class_iou_b, miou_b_final, acc_b_final = evaluate_segmentation(\n",
    "    seg_model_b, val_seg_loader, model_name=\"Model B (Semantic-Enhanced)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Per-Class IoU Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class IoU comparison bar chart\n",
    "class_names = [LOVEDA_CLASSES[i] for i in range(1, 7)]\n",
    "iou_a_vals = [per_class_iou_a.get(i, 0) for i in range(1, 7)]\n",
    "iou_b_vals = [per_class_iou_b.get(i, 0) for i in range(1, 7)]\n",
    "\n",
    "# Replace NaN with 0 for plotting\n",
    "iou_a_vals = [0 if np.isnan(v) else v for v in iou_a_vals]\n",
    "iou_b_vals = [0 if np.isnan(v) else v for v in iou_b_vals]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, iou_a_vals, width, label='Model A (Baseline)', alpha=0.85, color='orange')\n",
    "bars2 = ax.bar(x + width/2, iou_b_vals, width, label='Model B (Semantic)', alpha=0.85, color='green')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    h = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., h + 0.005, f'{h:.3f}',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    h = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., h + 0.005, f'{h:.3f}',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=12)\n",
    "ax.set_ylabel('IoU', fontsize=12)\n",
    "ax.set_title('Per-Class IoU Comparison (Segmentation)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names, fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add mIoU lines\n",
    "ax.axhline(y=miou_a_final, color='orange', linestyle='--', alpha=0.6, label=f'mIoU A: {miou_a_final:.4f}')\n",
    "ax.axhline(y=miou_b_final, color='green', linestyle='--', alpha=0.6, label=f'mIoU B: {miou_b_final:.4f}')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print improvement per class\n",
    "print(\"\\nPer-Class IoU Improvement:\")\n",
    "print(f\"{'Class':<15} {'Model A':>8} {'Model B':>8} {'Delta':>8}\")\n",
    "print(f\"{'-'*42}\")\n",
    "for i, name in enumerate(class_names):\n",
    "    delta = iou_b_vals[i] - iou_a_vals[i]\n",
    "    print(f\"{name:<15} {iou_a_vals[i]:>8.4f} {iou_b_vals[i]:>8.4f} {delta:>+8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Visual Results: Side-by-Side Predictions\n",
    "\n",
    "Display sample validation images with:\n",
    "1. Original image\n",
    "2. Ground truth mask\n",
    "3. Model A prediction\n",
    "4. Model B prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Reverse normalization for display.\"\"\"\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return (tensor.cpu() * std + mean).clamp(0, 1)\n",
    "\n",
    "\n",
    "# Select sample images from validation set\n",
    "num_samples = 4\n",
    "sample_indices = np.linspace(0, len(val_seg_dataset)-1, num_samples, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(num_samples, 4, figsize=(20, 5 * num_samples))\n",
    "\n",
    "seg_model_a.eval()\n",
    "seg_model_b.eval()\n",
    "\n",
    "for row, idx in enumerate(sample_indices):\n",
    "    image, gt_mask = val_seg_dataset[idx]\n",
    "    image_input = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        pred_a = seg_model_a(image_input).argmax(dim=1)[0].cpu().numpy()\n",
    "        pred_b = seg_model_b(image_input).argmax(dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    gt_mask_np = gt_mask.numpy()\n",
    "    \n",
    "    # Original image (denormalized, resized to 512 for display alignment)\n",
    "    img_display = denormalize(image).permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Column 0: Original Image\n",
    "    axes[row, 0].imshow(img_display)\n",
    "    axes[row, 0].set_title('Input Image' if row == 0 else '', fontsize=12, fontweight='bold')\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # Column 1: Ground Truth\n",
    "    axes[row, 1].imshow(colorize_mask(gt_mask_np))\n",
    "    axes[row, 1].set_title('Ground Truth' if row == 0 else '', fontsize=12, fontweight='bold')\n",
    "    axes[row, 1].axis('off')\n",
    "    \n",
    "    # Column 2: Model A prediction\n",
    "    axes[row, 2].imshow(colorize_mask(pred_a))\n",
    "    axes[row, 2].set_title('Model A (Baseline)' if row == 0 else '', fontsize=12, fontweight='bold')\n",
    "    axes[row, 2].axis('off')\n",
    "    \n",
    "    # Column 3: Model B prediction\n",
    "    axes[row, 3].imshow(colorize_mask(pred_b))\n",
    "    axes[row, 3].set_title('Model B (Semantic)' if row == 0 else '', fontsize=12, fontweight='bold')\n",
    "    axes[row, 3].axis('off')\n",
    "\n",
    "# Add legend at bottom\n",
    "legend_patches = []\n",
    "for cls_id in range(1, 7):\n",
    "    color = [c / 255.0 for c in LOVEDA_COLORS[cls_id]]\n",
    "    patch = mpatches.Patch(color=color, label=f\"{LOVEDA_CLASSES[cls_id]}\")\n",
    "    legend_patches.append(patch)\n",
    "\n",
    "fig.legend(handles=legend_patches, loc='lower center', ncol=6,\n",
    "           fontsize=11, frameon=True, fancybox=True, shadow=True,\n",
    "           bbox_to_anchor=(0.5, -0.01))\n",
    "\n",
    "plt.suptitle('Segmentation Predictions: Ground Truth vs Model A vs Model B',\n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Summary & Cross-Task Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n--- PHASE 1: Spatial Knowledge Distillation (Unsupervised) ---\")\n",
    "print(f\"  Teacher: RemoteCLIP ViT-B/32 (frozen) — patch tokens [B, 512, 7×7]\")\n",
    "print(f\"  Student: Prithvi EO v2 (frozen) + Spatial Projection Head (trainable)\")\n",
    "print(f\"  Alignment: Per-position cosine embedding loss\")\n",
    "print(f\"  Training Samples: {len(phase1_dataset):,}\")\n",
    "print(f\"  Epochs: {phase1_epochs}\")\n",
    "print(f\"  Best Loss: {best_phase1_loss:.4f}\")\n",
    "print(f\"  Best Cosine Similarity: {phase1_history['cosine_sim'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\n--- PHASE 2: Semantic Segmentation on LoveDA ---\")\n",
    "print(f\"  Classes: 7 (Background ignored in evaluation)\")\n",
    "print(f\"  Training Samples: {len(train_seg_dataset):,}\")\n",
    "print(f\"  Validation Samples: {len(val_seg_dataset):,}\")\n",
    "print(f\"  Epochs: 25\")\n",
    "\n",
    "print(f\"\\n{'─'*70}\")\n",
    "print(f\"{'RESULTS':^70}\")\n",
    "print(f\"{'─'*70}\")\n",
    "print(f\"  {'Metric':<25} {'Model A':>12} {'Model B':>12} {'Delta':>10}\")\n",
    "print(f\"  {'-'*60}\")\n",
    "print(f\"  {'mIoU':<25} {miou_a_final:>12.4f} {miou_b_final:>12.4f} {miou_b_final-miou_a_final:>+10.4f}\")\n",
    "print(f\"  {'Pixel Accuracy':<25} {acc_a_final:>11.2f}% {acc_b_final:>11.2f}% {acc_b_final-acc_a_final:>+9.2f}%\")\n",
    "\n",
    "print(f\"\\n{'─'*70}\")\n",
    "print(f\"{'CROSS-TASK COMPARISON':^70}\")\n",
    "print(f\"{'─'*70}\")\n",
    "print(f\"  {'Task':<30} {'Model A':>10} {'Model B':>10} {'Delta':>10}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "print(f\"  {'Classification (Accuracy)':<30} {'35.78%':>10} {'40.38%':>10} {'+4.60%':>10}\")\n",
    "print(f\"  {'Segmentation (mIoU)':<30} {miou_a_final:>10.4f} {miou_b_final:>10.4f} {miou_b_final-miou_a_final:>+10.4f}\")\n",
    "print(f\"  {'Segmentation (Pixel Acc)':<30} {acc_a_final:>9.2f}% {acc_b_final:>9.2f}% {acc_b_final-acc_a_final:>+9.2f}%\")\n",
    "\n",
    "print(f\"\\n{'─'*70}\")\n",
    "print(f\"KEY INSIGHTS:\")\n",
    "print(f\"{'─'*70}\")\n",
    "if miou_b_final > miou_a_final:\n",
    "    print(f\"  * Semantic enhancement via spatial RemoteCLIP alignment improved\")\n",
    "    print(f\"    segmentation by {miou_b_final-miou_a_final:+.4f} mIoU on the harder LoveDA task.\")\n",
    "    print(f\"  * The spatial projection head successfully transfers semantic knowledge\")\n",
    "    print(f\"    to per-pixel predictions, not just global image classification.\")\n",
    "    print(f\"  * This validates that RemoteCLIP's semantic understanding benefits\")\n",
    "    print(f\"    dense prediction tasks, confirming the generality of the approach.\")\n",
    "else:\n",
    "    print(f\"  * The baseline Prithvi model performed comparably or better on segmentation.\")\n",
    "    print(f\"  * Spatial feature compression (1024→512) may discard spatial detail\")\n",
    "    print(f\"    that is important for dense per-pixel prediction.\")\n",
    "    print(f\"  * The classification improvement (+4.60%) did not fully transfer\")\n",
    "    print(f\"    to the harder spatial reasoning task.\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
